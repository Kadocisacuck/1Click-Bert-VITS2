{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunbooshi/1Click-Bert-VITS2/blob/main/1ClickBertVITS2.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-nlHfzqf58H"
      },
      "source": [
        "# 环境准备\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4bxaVFnKiRIj"
      },
      "outputs": [],
      "source": [
        "#@title 安装Bert-VITS2\n",
        "!git clone https://github.com/fishaudio/Bert-VITS2\n",
        "%cd Bert-VITS2\n",
        "!git checkout aae043f0e39fe1dc85098ce5df56c4fa6243a1dd\n",
        "%pip install -r requirements.txt\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ydvX6nznf4xU"
      },
      "outputs": [],
      "source": [
        "#@title 安装audio-slicer\n",
        "!git clone https://github.com/openvpi/audio-slicer.git audio_slicer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zUKEMbS9gNl_"
      },
      "outputs": [],
      "source": [
        "#@title 安装whisper\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install torchaudio\n",
        "# 下载模型\n",
        "!wget -P /content/whisper/ https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 安装onnxruntime\n",
        "!pip install onnx onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j5Rl5ALhgXYi"
      },
      "outputs": [],
      "source": [
        "#@title 安装ffmpeg\n",
        "import os, uuid, re, IPython\n",
        "import ipywidgets as widgets\n",
        "import time\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import output, drive\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import os, sys, urllib.request\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "pathDoneCMD = f'{HOME}/doneCMD.sh'\n",
        "if not os.path.exists(f\"{HOME}/.ipython/ttmg.py\"):\n",
        "    hCode = \"https://raw.githubusercontent.com/yunooooo/gcct/master/res/ttmg.py\"\n",
        "    urllib.request.urlretrieve(hCode, f\"{HOME}/.ipython/ttmg.py\")\n",
        "\n",
        "from ttmg import (\n",
        "    loadingAn,\n",
        "    textAn,\n",
        ")\n",
        "\n",
        "loadingAn(name=\"lds\")\n",
        "textAn(\"Cloning Repositories...\", ty='twg')\n",
        "!git clone https://github.com/XniceCraft/ffmpeg-colab.git\n",
        "!chmod 755 ./ffmpeg-colab/install\n",
        "textAn(\"Installing FFmpeg...\", ty='twg')\n",
        "!./ffmpeg-colab/install\n",
        "clear_output()\n",
        "print('Installation finished!')\n",
        "!rm -fr /content/ffmpeg-colab\n",
        "!ffmpeg -version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tvcYwiQAggHe"
      },
      "outputs": [],
      "source": [
        "#@title 准备目录\n",
        "%cd /content/Bert-VITS2/\n",
        "import os\n",
        "def mkdir(path):\n",
        "  if os.path.exists(path):\n",
        "    print(f'{path} exists')\n",
        "  else:\n",
        "    os.mkdir(path)\n",
        "data_dir = 'Data'\n",
        "mkdir(data_dir)\n",
        "speaker = \"paimeng\" # @param {type:\"string\"}\n",
        "speaker_dir = os.path.join(data_dir, speaker)\n",
        "mkdir(speaker_dir)\n",
        "audios = os.path.join(speaker_dir, 'audios')\n",
        "mkdir(audios)\n",
        "audios_raw = os.path.join(audios, 'raw')\n",
        "mkdir(audios_raw)\n",
        "audios_wavs = os.path.join(audios, 'wavs')\n",
        "mkdir(audios_wavs)\n",
        "filelists = os.path.join(speaker_dir, 'filelists')\n",
        "mkdir(filelists)\n",
        "input_dir = \"inputs\"\n",
        "mkdir(input_dir)\n",
        "models_dir = os.path.join(speaker_dir, 'models')\n",
        "mkdir(models_dir)\n",
        "%env NLTK_DATA=/content/nltk_data\n",
        "!mkdir /content/nltk_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp1Q3Y1phS8y"
      },
      "source": [
        "# 模型下载"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5XQuP0IdHTkx"
      },
      "outputs": [],
      "source": [
        "#@title 下载模型\n",
        "!wget -P slm/wavlm-base-plus/ https://huggingface.co/microsoft/wavlm-base-plus/resolve/main/pytorch_model.bin\n",
        "!wget -P bert/chinese-roberta-wwm-ext-large/ https://huggingface.co/hfl/chinese-roberta-wwm-ext-large/resolve/main/pytorch_model.bin\n",
        "!wget -P bert/deberta-v3-large/ https://huggingface.co/microsoft/deberta-v3-large/resolve/main/pytorch_model.bin\n",
        "!wget -P bert/deberta-v3-large/ https://huggingface.co/microsoft/deberta-v3-large/resolve/main/pytorch_model.generator.bin\n",
        "!wget -P bert/deberta-v3-large/ https://huggingface.co/microsoft/deberta-v3-large/resolve/main/spm.model\n",
        "!wget -P bert/deberta-v2-large-japanese-char-wwm/ https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm/resolve/main/pytorch_model.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_LfuzFbHeib"
      },
      "outputs": [],
      "source": [
        "# @title 训练底模\n",
        "!wget -P $models_dir https://huggingface.co/OedoSoldier/Bert-VITS2-2.3/resolve/main/DUR_0.pth\n",
        "!wget -P $models_dir https://huggingface.co/OedoSoldier/Bert-VITS2-2.3/resolve/main/D_0.pth\n",
        "!wget -P $models_dir https://huggingface.co/OedoSoldier/Bert-VITS2-2.3/resolve/main/G_0.pth\n",
        "!wget -P $models_dir https://huggingface.co/OedoSoldier/Bert-VITS2-2.3/resolve/main/WD_0.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP139P4Qhe18"
      },
      "source": [
        "# 准备训练数据"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4hhxVLUhm1O"
      },
      "source": [
        "## 上传训练数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15b2TdwziBi8"
      },
      "outputs": [],
      "source": [
        "# @title ### 直接上传\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for f in uploaded.keys():\n",
        "  shutil.move(f, input_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wyfuXDueEeT"
      },
      "source": [
        "### 从GoogleDrive 复制"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LYlkjDLZhyM4"
      },
      "outputs": [],
      "source": [
        "# @title #### 挂载GoogleDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rkqEVVqrePkH"
      },
      "outputs": [],
      "source": [
        "# @title #### 复制文件\n",
        "import shutil\n",
        "# @markdown  输入要复制的文件路径：\n",
        "drive_path = \"/content/drive/MyDrive/data/paimeng.wav\" # @param {type:\"string\"}\n",
        "if os.path.isdir(drive_path):\n",
        "  !cp {drive_path}*.wav {input_dir}\n",
        "else:\n",
        "  !cp {drive_path} {input_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1eleBAgGFh3"
      },
      "source": [
        "## 数据处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "llFeG-tdIAwE"
      },
      "outputs": [],
      "source": [
        "# @title ### 音频分割\n",
        "import librosa  # Optional. Use any library you like to read audio files.\n",
        "import soundfile  # Optional. Use any library you like to write audio files.\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/audio_slicer')\n",
        "from slicer2 import Slicer\n",
        "\n",
        "def slice_audio(in_file, out_path, out_name, base_index=0):\n",
        "    audio, sr = librosa.load(in_file, sr=None, mono=False)  # Load an audio file with librosa.\n",
        "    slicer = Slicer(\n",
        "        sr=sr,\n",
        "        threshold=-40,\n",
        "        min_length=2000,\n",
        "        min_interval=300,\n",
        "        hop_size=10,\n",
        "        max_sil_kept=500\n",
        "    )\n",
        "    chunks = slicer.slice(audio)\n",
        "    total = 0\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        total = total + 1\n",
        "        if len(chunk.shape) > 1:\n",
        "            chunk = chunk.T  # Swap axes if the audio is stereo.\n",
        "        out_file = os.path.join(out_path, '%s_%03d.wav' % (out_name, base_index+i))\n",
        "        soundfile.write(out_file, chunk, sr)  # Save sliced audio files with soundfile.\n",
        "\n",
        "    return total\n",
        "\n",
        "def slice_dir(input_dir, output_dir, model_name):\n",
        "    # 列出当前工作目录中的所有文件和文件夹\n",
        "    files = os.listdir(input_dir)\n",
        "\n",
        "    base_index = 0\n",
        "    # 筛选出所有以 \".wav\" 结尾的文件\n",
        "    wav_files = [f for f in files if f.endswith(\".wav\")]\n",
        "\n",
        "    # 打印所有 wav 文件的名称\n",
        "    for f in wav_files:\n",
        "        base_index = base_index + slice_audio(os.path.join(input_dir, f), output_dir, model_name, base_index)\n",
        "\n",
        "\n",
        "if len(input_dir)>0 and len(audios_raw)>0 and len(speaker)>0:\n",
        "  slice_dir(input_dir, audios_raw, speaker)\n",
        "else:\n",
        "  print(\"error input\")\n",
        "\n",
        "print(\"分割完成\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fgpCz0pyHRpp"
      },
      "outputs": [],
      "source": [
        "# @title ### 音频标注\n",
        "\n",
        "import whisper\n",
        "import os\n",
        "import json\n",
        "import torchaudio\n",
        "import argparse\n",
        "import torch\n",
        "\n",
        "lang2token = {\n",
        "            'zh': \"ZH|\",\n",
        "            'ja': \"JP|\",\n",
        "            \"en\": \"EN|\",\n",
        "        }\n",
        "\n",
        "def transcribe(audio_path):\n",
        "    # load audio and pad/trim it to fit 30 seconds\n",
        "    audio = whisper.load_audio(audio_path)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    # make log-Mel spectrogram and move to the same device as the model\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    # detect the spoken language\n",
        "    _, probs = model.detect_language(mel)\n",
        "    print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
        "    lang = max(probs, key=probs.get)\n",
        "    # decode the audio\n",
        "    options = whisper.DecodingOptions(beam_size=5)\n",
        "    result = whisper.decode(model, mel, options)\n",
        "\n",
        "    # print the recognized text\n",
        "    print(result.text)\n",
        "    return lang, result.text\n",
        "\n",
        "\n",
        "# assert (torch.cuda.is_available()), \"Please enable GPU in order to run Whisper!\"\n",
        "model = whisper.load_model(\"medium\", download_root=\"/content/whisper\")\n",
        "speaker_annos = []\n",
        "total_files = sum([len(files) for r, d, files in os.walk(audios_raw)])\n",
        "processed_files = 0\n",
        "\n",
        "for i, wavfile in enumerate(list(os.walk(audios_raw))[0][2]):\n",
        "    # try to load file as audio\n",
        "    try:\n",
        "        lang, text = transcribe(os.path.join(audios_raw, wavfile))\n",
        "        if lang not in list(lang2token.keys()):\n",
        "            print(f\"{lang} not supported, ignoring\\n\")\n",
        "            continue\n",
        "\n",
        "        text = f\"{audios_wavs}/{wavfile}|\" + f\"{speaker}|\" +lang2token[lang] + text + \"\\n\"\n",
        "        speaker_annos.append(text)\n",
        "\n",
        "        processed_files += 1\n",
        "        print(f\"Processed: {processed_files}/{total_files}\")\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        continue\n",
        "\n",
        "if len(speaker_annos) == 0:\n",
        "    print(\"Warning: no short audios found, this IS expected if you have only uploaded long audios, videos or video links.\")\n",
        "    print(\"this IS NOT expected if you have uploaded a zip file of short audios. Please check your file structure or make sure your audio language is supported.\")\n",
        "with open(os.path.join(filelists, 'esd.list'), 'w', encoding='utf-8') as f:\n",
        "    for line in speaker_annos:\n",
        "        f.write(line)\n",
        "\n",
        "print(\"标注完成\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BBzWZmTjfhei"
      },
      "outputs": [],
      "source": [
        "# @title ### 配置文件生成\n",
        "log_interval = 50 # @param {type:\"integer\"}\n",
        "eval_interval = 100 # @param {type:\"integer\"}\n",
        "epochs = 100 # @param {type:\"integer\"}\n",
        "batch_size = 4 # @param {type:\"integer\"}\n",
        "bert_device = \"cuda\" # @param [\"cuda\", \"cpu\"]\n",
        "bert_processs = 2 # @param {type:\"integer\"}\n",
        "\n",
        "import json\n",
        "with open('configs/config.json') as fp:\n",
        "  configs = json.load(fp)\n",
        "\n",
        "configs['train']['log_interval'] = log_interval\n",
        "configs['train']['eval_interval'] = eval_interval\n",
        "configs['train']['epochs'] = epochs\n",
        "configs['train']['batch_size'] = batch_size\n",
        "\n",
        "configs['data']['training_files'] = os.path.join(filelists, 'train.list')\n",
        "configs['data']['validation_files'] = os.path.join(filelists, 'val.list')\n",
        "configs['data']['n_speakers'] = 1\n",
        "configs['data']['spk2id'] = {speaker : 0}\n",
        "\n",
        "configs_path = os.path.join(speaker_dir, 'config.json')\n",
        "with open(configs_path, 'w') as fp:\n",
        "  json.dump(configs, fp, indent=4)\n",
        "  print(f'生成配置文件: {configs_path}')\n",
        "\n",
        "configs = None\n",
        "import yaml\n",
        "with open('default_config.yml') as fp:\n",
        "  configs = yaml.load(fp, Loader=yaml.FullLoader)\n",
        "\n",
        "configs['dataset_path'] = speaker_dir\n",
        "configs['preprocess_text']['transcription_path'] = 'filelists/esd.list'\n",
        "configs['bert_gen']['device'] = bert_device\n",
        "configs['bert_gen']['num_processes'] = bert_processs\n",
        "\n",
        "with open('config.yml', 'w') as fp:\n",
        "  yaml.dump(configs, fp)\n",
        "  print(f'生成配置文件: config.yml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4XFx-6PLEWER"
      },
      "outputs": [],
      "source": [
        "#@title 音频重采样\n",
        "!python resample.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CMZG8FyzEc-T"
      },
      "outputs": [],
      "source": [
        "#@title 生成训练集、验证集\n",
        "!python preprocess_text.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZJImUsCcEjWH"
      },
      "outputs": [],
      "source": [
        "#@title 生成Bert\n",
        "!python bert_gen.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eNwA_teGWJO"
      },
      "source": [
        "# Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7Q6byqrGVP-"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir $models_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm82HrCCEsGc"
      },
      "source": [
        "# 训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvwcObnfEukD"
      },
      "outputs": [],
      "source": [
        "!python train_ms.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd0z3mBRE22P"
      },
      "source": [
        "# 推理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "38Ns5KRGE8zW"
      },
      "outputs": [],
      "source": [
        "#@title 设置推理模型\n",
        "model = \"G_500.pth\" # @param {type:\"string\"}\n",
        "\n",
        "configs = None\n",
        "import yaml\n",
        "with open('config.yml') as fp:\n",
        "  configs = yaml.load(fp, Loader=yaml.FullLoader)\n",
        "  configs['webui']['model'] = f'models/{model}'\n",
        "  configs['webui']['share'] = True\n",
        "\n",
        "with open('config.yml', 'w') as fp:\n",
        "  yaml.dump(configs, fp)\n",
        "  print(f'更新配置文件: config.yml')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spBk2O0HZpb7"
      },
      "source": [
        "## WebUI推理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rlu7blKoZZuc"
      },
      "outputs": [],
      "source": [
        "#@title 启动WebUI\n",
        "print('启动webui')\n",
        "!python webui.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "J6Xr8Xe7zKno"
      },
      "outputs": [],
      "source": [
        "#@title 修复缺少frpc_linux_amd64提示\n",
        "!wget https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64\n",
        "!chmod +x frpc_linux_amd64\n",
        "!mv frpc_linux_amd64 /usr/local/lib/python3.10/dist-packages/gradio/frpc_linux_amd64_v0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvOpK_KaZu1D"
      },
      "source": [
        "## 直接推理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "X7moVF1AHIC3"
      },
      "outputs": [],
      "source": [
        "#@title 载入模型\n",
        "import gc\n",
        "import os\n",
        "import logging\n",
        "import re_matching\n",
        "from tools.sentence import split_by_language\n",
        "\n",
        "import scipy\n",
        "import torch\n",
        "from config import config\n",
        "from infer import infer, latest_version, get_net_g, infer_multilang\n",
        "import gradio as gr\n",
        "import utils\n",
        "from IPython.display import Audio\n",
        "\n",
        "net_g = None\n",
        "device = 'cuda'\n",
        "hps = None\n",
        "\n",
        "def free_up_memory():\n",
        "    # Prior inference run might have large variables not cleaned up due to exception during the run.\n",
        "    # Free up as much memory as possible to allow this run to be successful.\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def process_mix(slice):\n",
        "    _speaker = slice.pop()\n",
        "    _text, _lang = [], []\n",
        "    for lang, content in slice:\n",
        "        content = content.split(\"|\")\n",
        "        content = [part for part in content if part != \"\"]\n",
        "        if len(content) == 0:\n",
        "            continue\n",
        "        if len(_text) == 0:\n",
        "            _text = [[part] for part in content]\n",
        "            _lang = [[lang] for part in content]\n",
        "        else:\n",
        "            _text[-1].append(content[0])\n",
        "            _lang[-1].append(lang)\n",
        "            if len(content) > 1:\n",
        "                _text += [[part] for part in content[1:]]\n",
        "                _lang += [[lang] for part in content[1:]]\n",
        "    return _text, _lang, _speaker\n",
        "\n",
        "def process_auto(text):\n",
        "    _text, _lang = [], []\n",
        "    for slice in text.split(\"|\"):\n",
        "        if slice == \"\":\n",
        "            continue\n",
        "        temp_text, temp_lang = [], []\n",
        "        sentences_list = split_by_language(slice, target_languages=[\"zh\", \"ja\", \"en\"])\n",
        "        for sentence, lang in sentences_list:\n",
        "            if sentence == \"\":\n",
        "                continue\n",
        "            temp_text.append(sentence)\n",
        "            if lang == \"ja\":\n",
        "                lang = \"jp\"\n",
        "            temp_lang.append(lang.upper())\n",
        "        _text.append(temp_text)\n",
        "        _lang.append(temp_lang)\n",
        "    return _text, _lang\n",
        "\n",
        "def format_utils(text, speaker):\n",
        "    _text, _lang = process_auto(text)\n",
        "    res = f\"[{speaker}]\"\n",
        "    for lang_s, content_s in zip(_lang, _text):\n",
        "        for lang, content in zip(lang_s, content_s):\n",
        "            res += f\"<{lang.lower()}>{content}\"\n",
        "        res += \"|\"\n",
        "    return \"mix\", res[:-1]\n",
        "\n",
        "def generate_audio(\n",
        "    slices,\n",
        "    sdp_ratio=0.5,\n",
        "    noise_scale=0.5,\n",
        "    noise_scale_w=0.9,\n",
        "    length_scale=1.0,\n",
        "    speaker=\"paimeng\",\n",
        "    language=\"mix\",\n",
        "    reference_audio=None,\n",
        "    emotion=\"Happy\",\n",
        "    style_text=None,\n",
        "    style_weight=0.7,\n",
        "    skip_start=False,\n",
        "    skip_end=False,\n",
        "):\n",
        "    audio_list = []\n",
        "    # silence = np.zeros(hps.data.sampling_rate // 2, dtype=np.int16)\n",
        "    global hps, device, net_g\n",
        "    free_up_memory()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, piece in enumerate(slices):\n",
        "            skip_start = idx != 0\n",
        "            skip_end = idx != len(slices) - 1\n",
        "            audio = infer(\n",
        "                piece,\n",
        "                reference_audio=reference_audio,\n",
        "                emotion=emotion,\n",
        "                sdp_ratio=sdp_ratio,\n",
        "                noise_scale=noise_scale,\n",
        "                noise_scale_w=noise_scale_w,\n",
        "                length_scale=length_scale,\n",
        "                sid=speaker,\n",
        "                language=language,\n",
        "                hps=hps,\n",
        "                net_g=net_g,\n",
        "                device=device,\n",
        "                skip_start=skip_start,\n",
        "                skip_end=skip_end,\n",
        "                style_text=style_text,\n",
        "                style_weight=style_weight,\n",
        "            )\n",
        "            audio16bit = gr.processing_utils.convert_to_16_bit_wav(audio)\n",
        "            audio_list.append(audio16bit)\n",
        "    return audio_list\n",
        "\n",
        "def generate_audio_multilang(\n",
        "    slices,\n",
        "    sdp_ratio,\n",
        "    noise_scale,\n",
        "    noise_scale_w,\n",
        "    length_scale,\n",
        "    speaker,\n",
        "    language,\n",
        "    reference_audio,\n",
        "    emotion,\n",
        "    skip_start=False,\n",
        "    skip_end=False,\n",
        "):\n",
        "    audio_list = []\n",
        "    # silence = np.zeros(hps.data.sampling_rate // 2, dtype=np.int16)\n",
        "\n",
        "    free_up_memory()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, piece in enumerate(slices):\n",
        "            skip_start = idx != 0\n",
        "            skip_end = idx != len(slices) - 1\n",
        "            audio = infer_multilang(\n",
        "                piece,\n",
        "                reference_audio=reference_audio,\n",
        "                emotion=emotion,\n",
        "                sdp_ratio=sdp_ratio,\n",
        "                noise_scale=noise_scale,\n",
        "                noise_scale_w=noise_scale_w,\n",
        "                length_scale=length_scale,\n",
        "                sid=speaker,\n",
        "                language=language[idx],\n",
        "                hps=hps,\n",
        "                net_g=net_g,\n",
        "                device=device,\n",
        "                skip_start=skip_start,\n",
        "                skip_end=skip_end,\n",
        "            )\n",
        "            audio16bit = gr.processing_utils.convert_to_16_bit_wav(audio)\n",
        "            audio_list.append(audio16bit)\n",
        "    return audio_list\n",
        "\n",
        "def process_text(\n",
        "    text: str,\n",
        "    speaker='paimeng',\n",
        "    sdp_ratio=0.5,\n",
        "    noise_scale=0.6,\n",
        "    noise_scale_w=0.9,\n",
        "    length_scale=1.0,\n",
        "    language='mix',\n",
        "    reference_audio=None,\n",
        "    emotion='Happy',\n",
        "    style_text=None,\n",
        "    style_weight=0,\n",
        "):\n",
        "    audio_list = []\n",
        "    if language == \"mix\":\n",
        "        bool_valid, str_valid = re_matching.validate_text(text)\n",
        "        if not bool_valid:\n",
        "            return str_valid, (\n",
        "                hps.data.sampling_rate,\n",
        "                np.concatenate([np.zeros(hps.data.sampling_rate // 2)]),\n",
        "            )\n",
        "        for slice in re_matching.text_matching(text):\n",
        "            _text, _lang, _speaker = process_mix(slice)\n",
        "            if _speaker is None:\n",
        "                continue\n",
        "            print(f\"Text: {_text}\\nLang: {_lang}\")\n",
        "            audio_list.extend(\n",
        "                generate_audio_multilang(\n",
        "                    _text,\n",
        "                    sdp_ratio,\n",
        "                    noise_scale,\n",
        "                    noise_scale_w,\n",
        "                    length_scale,\n",
        "                    _speaker,\n",
        "                    _lang,\n",
        "                    reference_audio,\n",
        "                    emotion,\n",
        "                )\n",
        "            )\n",
        "    elif language.lower() == \"auto\":\n",
        "        _text, _lang = process_auto(text)\n",
        "        print(f\"Text: {_text}\\nLang: {_lang}\")\n",
        "        audio_list.extend(\n",
        "            generate_audio_multilang(\n",
        "                _text,\n",
        "                sdp_ratio,\n",
        "                noise_scale,\n",
        "                noise_scale_w,\n",
        "                length_scale,\n",
        "                speaker,\n",
        "                _lang,\n",
        "                reference_audio,\n",
        "                emotion,\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        audio_list.extend(\n",
        "            generate_audio(\n",
        "                text.split(\"|\"),\n",
        "                sdp_ratio,\n",
        "                noise_scale,\n",
        "                noise_scale_w,\n",
        "                length_scale,\n",
        "                speaker,\n",
        "                language,\n",
        "                reference_audio,\n",
        "                emotion,\n",
        "                style_text,\n",
        "                style_weight,\n",
        "            )\n",
        "        )\n",
        "    return audio_list\n",
        "\n",
        "def init_model():\n",
        "    global hps, net_g, device\n",
        "    hps = utils.get_hparams_from_file(config.webui_config.config_path)\n",
        "    # 若config.json中未指定版本则默认为最新版本\n",
        "    version = hps.version if hasattr(hps, \"version\") else latest_version\n",
        "    model_path = os.path.join(models_dir, model)\n",
        "    print('load model: ' + model_path)\n",
        "    net_g = get_net_g(model_path=model_path, version=version, device=device, hps=hps)\n",
        "\n",
        "def text_to_audio(text, speaker, sdp_ratio, noise_scale, noise_scale_w, length_scale, language, filename):\n",
        "    audios = process_text(text, speaker, sdp_ratio, noise_scale, noise_scale_w, length_scale, language, None, 'Happy')\n",
        "    if len(audios) == 1:\n",
        "        scipy.io.wavfile.write(filename, 44100, audios[0])\n",
        "    else:\n",
        "        print('生成失败')\n",
        "\n",
        "init_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "InjNnAjwFFz7"
      },
      "outputs": [],
      "source": [
        "#@title 直接推理\n",
        "text = \"大家好，这里是孙博士研究所！\" # @param {type:\"string\"}\n",
        "sdp_ratio = 0.5 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "noise_scale = 0.6 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "noise_scale_w = 0.9 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "length_scale =  1.0 # @param {type:\"slider\", min:0.1, max:2, step:0.1}\n",
        "language = \"ZH\" # @param [\"ZH\", \"JP\", \"EN\", \"mix\", \"auto\"]\n",
        "filename = \"output.wav\" # @param {type:\"string\"}\n",
        "\n",
        "text_to_audio(text,\n",
        "              speaker,\n",
        "              sdp_ratio,\n",
        "              noise_scale,\n",
        "              noise_scale_w,\n",
        "              length_scale,\n",
        "              language,\n",
        "              os.path.join('/content', filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rSAyReahlFJY"
      },
      "outputs": [],
      "source": [
        "#@title 播放音频\n",
        "Audio(filename=os.path.join('/content', filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKRdeeKWe9o4"
      },
      "source": [
        "# 训练数据保存&恢复"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 训练数据保存路径\n",
        "data_save_dir = \"/content/drive/MyDrive/data/save\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 恢复训练数据\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print('删除Data目录')\n",
        "!rm -fr {data_dir}\n",
        "print('恢复训练数据中...')\n",
        "!rsync -r --info=progress2 --info=name0 {data_save_dir}/Data /content/Bert-VITS2\n",
        "!cp {data_dir}/config.yml /content/Bert-VITS2/config.yml\n",
        "print('恢复完成')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfI_1qCCfBAn"
      },
      "outputs": [],
      "source": [
        "#@title 保存训练数据\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print('模型保存中...')\n",
        "!cp /content/Bert-VITS2/config.yml {data_dir}\n",
        "!rsync -r --info=progress2 --info=name0 {data_dir} {data_save_dir}\n",
        "print('保存完成！')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 导出ONNX模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 导出\n",
        "from onnx_modules import export_onnx\n",
        "import os\n",
        "\n",
        "export_model = \"G_500.pth\" # @param {type:\"string\"}\n",
        "Extra = \"chinese\"  # @param [\"chinese\", \"japanese\"]\n",
        "model_path = os.path.join(models_dir, export_model)\n",
        "config_path = os.path.join(speaker_dir, 'config.json')\n",
        "novq = False\n",
        "dev = False\n",
        "print(model_path)\n",
        "\n",
        "if not os.path.exists(\"onnx\"):\n",
        "    os.makedirs(\"onnx\")\n",
        "if not os.path.exists(f\"onnx/{speaker}\"):\n",
        "    os.makedirs(f\"onnx/{speaker}\")\n",
        "export_onnx(speaker, model_path, config_path, novq, dev, Extra)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 保存ONNX模型到网盘\n",
        "onnx_save_dir = \"/content/drive/MyDrive/data/onnx_model\" # @param {type:\"string\"}\n",
        "print('模型保存中...')\n",
        "onnx_model_dir = \"/content/Bert-VITS2/onnx\"\n",
        "!rsync -r --info=progress2 --info=name0 {onnx_model_dir} {onnx_save_dir}\n",
        "print('保存完成！')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "K-nlHfzqf58H",
        "yp1Q3Y1phS8y",
        "lN0lGQHBekHz",
        "RP139P4Qhe18",
        "h4hhxVLUhm1O",
        "1eNwA_teGWJO",
        "Cm82HrCCEsGc",
        "qd0z3mBRE22P",
        "spBk2O0HZpb7",
        "SvOpK_KaZu1D",
        "xKRdeeKWe9o4"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
